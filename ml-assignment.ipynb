{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing important modules and setting up the project"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\n\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\nimport xgboost\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the data from files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/my-sample-data/train.csv')\ntest = pd.read_csv('../input/my-sample-data/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor_cols = ['additional_fare', 'duration', 'meter_waiting', 'meter_waiting_fare', 'meter_waiting_till_pickup', 'fare']\ntrain_X = train[predictor_cols]\ntest_X = test[predictor_cols]\n\n#train_y.fillna(train_y.mean(), inplace=True)\ntrain_X.fillna(train_X.mean(), inplace=True)\ntest_X.fillna(test_X.mean(), inplace=True)\ntrain['num_label'] = False\ntrain['direct_distance'] = 0.0\ntest['direct_distance'] = 0.0\n\nfor index, row in train.iterrows():\n    train.at[index,'direct_distance'] = ((((row['drop_lon'] - row['pick_lon'])**2)+((row['drop_lat'] - row['pick_lat'])**2))**0.5)*111\n    \nfor index, row in test.iterrows():\n    test.at[index,'direct_distance'] = ((((row['drop_lon'] - row['pick_lon'])**2)+((row['drop_lat'] - row['pick_lat'])**2))**0.5)*111\n\nfor index, row in train.iterrows():\n    if(row['label'] == \"correct\"):\n        train.at[index, 'num_label'] = 1\n    else:\n        train.at[index, 'num_label'] = 0\n\npredictor_cols = ['additional_fare', 'direct_distance', 'duration', 'meter_waiting', 'meter_waiting_fare', 'meter_waiting_till_pickup', 'fare']\ntrain_X_ = train[predictor_cols]\ntest_X = test[predictor_cols]\ntrain_X_.fillna(train_X.mean(), inplace=True)\ntest_X.fillna(test_X.mean(), inplace=True)\n\ntrain_y_ = train.num_label\n#train_y = train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(train_X_, train_y_, test_size = 0.2, train_size = 0.8, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    mse = mean_squared_error(test_labels, predictions)\n    mae = mean_absolute_error(test_labels, predictions)\n    rmse = np.sqrt(mse)\n    score=cross_val_score(model, test_features, test_labels, cv=10)\n    accuracy = accuracy_score(test_labels, predictions.round())\n    print('Model Performance')\n    print('Accuracy:%f'%accuracy)\n    print(\"Mean cross validation score:%f\"%score.mean())\n    print('Mean Squared Error : %.4f' % mse)\n    print('Root MSE : %.4f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_random_forest_model = RandomForestRegressor(n_estimators=100, random_state = 0)\nmy_random_forest_model.fit(train_X, train_y)\n\nevaluate(my_random_forest_model, val_X, val_y)\n\npred_classes = my_random_forest_model.predict(val_X)\npredi = []\nfor x in range(len(pred_classes)):\n    predi.append(bool(pred_classes[x]))\n    \npred_classes = np.array(predi)\n\nf1 = f1_score(val_y, pred_classes)\nprint('F1 score: %f' % f1)\n# Multiply by -1 since sklearn calculates *negative* MAE\n#scores = cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n#scores = cross_val_score(my_random_forest_model, train_X, train_y.round(), scoring='f1')\n#print(\"MAE scores:\\n\", scores)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision tree model"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_decision_tree_model = DecisionTreeRegressor(min_samples_split=100,\n        max_features=\"auto\", random_state=50, \n        max_depth=100)\nmy_decision_tree_model.fit(train_X, train_y)\n\nevaluate(my_decision_tree_model, val_X, val_y)\n\npred_classes = my_decision_tree_model.predict(val_X)\npredi = []\nfor x in range(len(pred_classes)):\n    predi.append(bool(pred_classes[x]))\n    \npred_classes = np.array(predi)\n\nf1 = f1_score(val_y, pred_classes)\nprint('F1 score: %f' % f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ada_boost_model = AdaBoostRegressor()\nAda_boost_model.fit(train_X, train_y)\n\nevaluate(Ada_boost_model, val_X, val_y)\n\npred_classes = Ada_boost_model.predict(val_X)\npredi = []\nfor x in range(len(pred_classes)):\n    predi.append(bool(pred_classes[x]))\n    \npred_classes = np.array(predi)\n\nf1 = f1_score(val_y, pred_classes)\nprint('F1 score: %f' % f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def create_baseline():\n'''\nmodel = Sequential()\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='relu'))\n#model.add(Dropout(0.4))\nmodel.add(Dense(1, activation='softmax'))\n#model.add(Dense(1, activation='sigmoid'))\n#model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\nmodel.compile(loss='mse', optimizer='Adam', metrics=[tf.keras.metrics.MeanSquaredError(),tf.keras.metrics.Accuracy()])\n#tf.keras.metrics.BinaryAccuracy()\n#adam,sgd\n#return model\n'''\nfrom keras.layers import BatchNormalization\nmodel = Sequential()\nmodel.add(Dense(7))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(16,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(8,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='mse', metrics=['accuracy'])\n#model.compile(loss=keras.losses.categorical_crossentropy,optimizer='adam',metrics=['accuracy'])\n\n#estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=10, verbose=0)\n#kfold = StratifiedKFold(n_splits=10, shuffle=True)\n#results = cross_val_score(estimator, train_X, train_y, cv=kfold)\n#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\nmodel.fit(train_X_.values, train_y_.values,\n          batch_size=10,\n          epochs=30,\n          validation_split = 0.2,\n          verbose = 1,\n          shuffle=True)\n\n#evaluate(model, val_X, val_y)\npred_classes = model.predict(val_X)\npredi = []\nfor x in range(len(pred_classes)):\n    predi.append(bool(pred_classes[x]))\n    \npred_classes = np.array(predi)\n\nf1 = f1_score(val_y, pred_classes)\nprint('F1 score: %f' % f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_model = xgboost.XGBClassifier(base_score=0.1, booster= None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n              gamma=0.4, gpu_id=0, importance_type='gain',\n              interaction_constraints=None, learning_rate=0.01,\n              max_delta_step=0, max_depth=80, min_child_weight=7, missing=None,\n              monotone_constraints=None, n_estimators=1000, n_jobs=6, nthread=6,\n              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, silent=False,\n              subsample=0.8,tree_method='hist', validate_parameters=False,\n              verbosity=1)\n#xg_model = xgboost.XGBClassifier()\n#base_score 0.5 -> 0.1\n#score=cross_val_score(xg_model, train_X, train_y, cv=10)\nxg_model.fit(train_X, train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xg_evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    #mse = mean_squared_error(test_labels, predictions)\n    #mae = mean_absolute_error(test_labels, predictions)\n    #rmse = np.sqrt(mse)\n    #score=cross_val_score(model, test_features, test_labels, cv=10)\n    accuracy = accuracy_score(test_labels, predictions.round())\n    #print('Model Performance')\n    print('Accuracy:%f'%accuracy)\n    #print(\"Mean cross validation score:%f\"%score.mean())\n    #print('Mean Squared Error : %.4f' % mse)\n    #print('Root MSE : %.4f' % rmse)\nxg_evaluate(xg_model, val_X, val_y)\npred_classes = xg_model.predict(val_X)\npredi = []\nfor x in range(len(pred_classes)):\n    predi.append(bool(pred_classes[x]))\n    \npred_classes = np.array(predi)\n\nf1 = f1_score(val_y, pred_classes)\nprint('F1 score: %f' % f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors=7)\nknn_classifier.fit(train_X, train_y)\n\ndef knn_evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    #mse = mean_squared_error(test_labels, predictions)\n    #mae = mean_absolute_error(test_labels, predictions)\n    #rmse = np.sqrt(mse)\n    #score=cross_val_score(model, test_features, test_labels, cv=10)\n    accuracy = accuracy_score(test_labels, predictions.round())\n    #print('Model Performance')\n    print('Accuracy:%f'%accuracy)\n    #print(\"Mean cross validation score:%f\"%score.mean())\n    #print('Mean Squared Error : %.4f' % mse)\n    #print('Root MSE : %.4f' % rmse)\nknn_evaluate(knn_classifier, val_X, val_y)\n\npred_classes = knn_classifier.predict(val_X)\npredi = []\nfor x in range(len(pred_classes)):\n    predi.append(bool(pred_classes[x]))\n    \npred_classes = np.array(predi)\n\nf1 = f1_score(val_y, pred_classes)\nprint('F1 score: %f' % f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_rf = my_random_forest_model.predict(test_X)\npredicted_dt = my_decision_tree_model.predict(test_X)\npredicted_ad = Ada_boost_model.predict(test_X)\npredicted_xg = xg_model.predict(test_X)\npredicted_NN = model.predict(test_X)\npredicted_KNN = knn_classifier.predict(test_X)\n\npred_KNN = []\nfor x in range(len(predicted_KNN)):\n    pred_KNN.append(bool(predicted_KNN[x]))\n\npredicted_NN = np.array(pred_NN)\n\npred_NN = []\nfor x in range(len(predicted_NN)):\n    pred_NN.append(bool(predicted_NN[x]))\n\npredicted_NN = np.array(pred_NN)\n    \npred_rf = []\nfor x in range(len(predicted_rf)):\n    pred_rf.append(bool(predicted_rf[x]))\n    \npredicted_rf = np.array(pred_rf)\n    \npred_dt = []\nfor x in range(len(predicted_dt)):\n    pred_dt.append(bool(predicted_dt[x]))\n\npredicted_dt = np.array(pred_dt)\n\npred_ad = []\nfor x in range(len(predicted_ad)):\n    pred_ad.append(bool(predicted_ad[x]))\n\npredicted_ad = np.array(pred_ad)\n\npred_xg = []\nfor x in range(len(predicted_xg)):\n    pred_xg.append(bool(predicted_xg[x]))\n    \npredicted_xg = np.array(pred_xg)\n\noutput_rf = pd.DataFrame({'tripid': test.tripid,\n                       'prediction': predicted_rf})\n\noutput_dt = pd.DataFrame({'tripid': test.tripid,\n                       'prediction': predicted_dt})\n\noutput_adb = pd.DataFrame({'tripid': test.tripid,\n                       'prediction': predicted_dt})\n\noutput_xg = pd.DataFrame({'tripid': test.tripid,\n                       'prediction': predicted_xg})\n\noutput_NN = pd.DataFrame({'tripid': test.tripid,\n                       'prediction': predicted_NN})\n\noutput_KNN = pd.DataFrame({'tripid': test.tripid,\n                       'prediction': predicted_KNN})\n\noutput_rf_path = \"submission_rf.csv\"\noutput_dt_path = \"submission_dt.csv\"\noutput_adb_path = \"submission_adb.csv\"\noutput_xg_path = \"submission_xg.csv\"\noutput_NN_path = \"submission_nn.csv\"\noutput_KNN_path = \"submission_knn.csv\"\n\noutput_rf.to_csv(output_rf_path, index=False)\noutput_dt.to_csv(output_dt_path, index=False)\noutput_adb.to_csv(output_adb_path, index = False)\noutput_xg.to_csv(output_xg_path, index = False)\noutput_NN.to_csv(output_NN_path, index = False)\noutput_KNN.to_csv(output_KNN_path, index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}